{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "W7hg0VQ4FbaF"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import poisson\n",
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Action(IntEnum):\n",
        "    \"\"\"Action\"\"\"\n",
        "\n",
        "    LEFT = 0\n",
        "    DOWN = 1\n",
        "    RIGHT = 2\n",
        "    UP = 3\n",
        "\n",
        "\n",
        "def actions_to_dxdy(action: Action) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Helper function to map action to changes in x and y coordinates\n",
        "    Args:\n",
        "        action (Action): taken action\n",
        "    Returns:\n",
        "        dxdy (Tuple[int, int]): Change in x and y coordinates\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        Action.LEFT: (-1, 0),\n",
        "        Action.DOWN: (0, -1),\n",
        "        Action.RIGHT: (1, 0),\n",
        "        Action.UP: (0, 1),\n",
        "    }\n",
        "    return mapping[action]\n",
        "\n",
        "\n",
        "class Gridworld5x5:\n",
        "    \"\"\"5x5 Gridworld\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        State: (x, y) coordinates\n",
        "\n",
        "        Actions: See class(Action).\n",
        "        \"\"\"\n",
        "        self.rows = 5\n",
        "        self.cols = 5\n",
        "        self.state_space = [\n",
        "            (x, y) for x in range(0, self.rows) for y in range(0, self.cols)\n",
        "        ]\n",
        "        self.action_space = len(Action)\n",
        "\n",
        "        # TODO set the locations of A and B, the next locations, and their rewards\n",
        "        self.A = (1, 4)\n",
        "        self.A_prime = (1, 0)\n",
        "        self.A_reward = 10\n",
        "        self.B = (3, 4)\n",
        "        self.B_prime = (3, 2)\n",
        "        self.B_reward = 5\n",
        "\n",
        "    def transitions(\n",
        "        self, state: Tuple, action: Action\n",
        "    ) -> Tuple[Tuple[int, int], float]:\n",
        "        \"\"\"Get transitions from given (state, action) pair.\n",
        "\n",
        "        Note that this is the 4-argument transition version p(s',r|s,a).\n",
        "        This particular environment has deterministic transitions\n",
        "\n",
        "        Args:\n",
        "            state (Tuple): state\n",
        "            action (Action): action\n",
        "\n",
        "        Returns:\n",
        "            next_state: Tuple[int, int]\n",
        "            reward: float\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # TODO Check if current state is A and B and return the next state and corresponding reward\n",
        "        # Else, check if the next step is within boundaries and return next state and reward\n",
        "\n",
        "        if state == self.A:\n",
        "            next_state = self.A_prime\n",
        "            reward = self.A_reward\n",
        "        elif state == self.B:\n",
        "            next_state = self.B_prime\n",
        "            reward = self.B_reward\n",
        "        else:\n",
        "            move = actions_to_dxdy(action)\n",
        "            next_state = (state[0] + move[0], state[1] + move[1])\n",
        "            reward = 0\n",
        "            if (next_state[0] not in range(5)) or (next_state[1] not in range(5)):\n",
        "                next_state = state\n",
        "                reward = -1\n",
        "\n",
        "        return next_state, reward\n",
        "\n",
        "    def expected_return(\n",
        "        self, V, state: Tuple[int, int], action: Action, gamma: float\n",
        "    ) -> float:\n",
        "        \"\"\"Compute the expected_return for all transitions from the (s,a) pair, i.e. do a 1-step Bellman backup.\n",
        "\n",
        "        Args:\n",
        "            V (np.ndarray): list of state values (length = number of states)\n",
        "            state (Tuple[int, int]): state\n",
        "            action (Action): action\n",
        "            gamma (float): discount factor\n",
        "\n",
        "        Returns:\n",
        "            ret (float): the expected return\n",
        "        \"\"\"\n",
        "\n",
        "        next_state, reward = self.transitions(state, action)\n",
        "        # TODO compute the expected return\n",
        "        ret = (1 / self.action_space) * (reward + (gamma * V[next_state]))\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class JacksCarRental:\n",
        "    def __init__(self, modified: bool = False) -> None:\n",
        "        \"\"\"JacksCarRental\n",
        "\n",
        "        Args:\n",
        "           modified (bool): False = original problem Q6a, True = modified problem for Q6b\n",
        "\n",
        "        State: tuple of (# cars at location A, # cars at location B)\n",
        "\n",
        "        Action (int): -5 to +5\n",
        "            Positive if moving cars from location A to B\n",
        "            Negative if moving cars from location B to A\n",
        "        \"\"\"\n",
        "        self.modified = modified\n",
        "\n",
        "        self.action_space = list(range(-5, 6))\n",
        "\n",
        "        self.rent_reward = 10\n",
        "        self.move_cost = 2\n",
        "\n",
        "        # For modified problem\n",
        "        self.overflow_cars = 10\n",
        "        self.overflow_cost = 4\n",
        "\n",
        "        # Rent and return Poisson process parameters\n",
        "        # Save as an array for each location (Loc A, Loc B)\n",
        "        self.rent = [poisson(3), poisson(4)]\n",
        "        self.return_ = [poisson(3), poisson(2)]\n",
        "\n",
        "        # Max number of cars at end of day\n",
        "        self.max_cars_end = 20\n",
        "        # Max number of cars at start of day\n",
        "        self.max_cars_start = self.max_cars_end + max(self.action_space)\n",
        "\n",
        "        self.state_space = [\n",
        "            (x, y)\n",
        "            for x in range(0, self.max_cars_end + 1)\n",
        "            for y in range(0, self.max_cars_end + 1)\n",
        "        ]\n",
        "\n",
        "        # Store all possible transitions here as a multi-dimensional array (locA, locB, action, locA', locB')\n",
        "        # This is the 3-argument transition function p(s'|s,a)\n",
        "        self.t = np.zeros(\n",
        "            (\n",
        "                self.max_cars_end + 1,\n",
        "                self.max_cars_end + 1,\n",
        "                len(self.action_space),\n",
        "                self.max_cars_end + 1,\n",
        "                self.max_cars_end + 1,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Store all possible rewards (locA, locB, action)\n",
        "        # This is the reward function r(s,a)\n",
        "        self.r = np.zeros(\n",
        "            (self.max_cars_end + 1, self.max_cars_end + 1, len(self.action_space))\n",
        "        )\n",
        "\n",
        "    def _open_to_close(self, loc_idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Computes the probability of ending the day with s_end \\in [0,20] cars given that the location started with s_start \\in [0, 20+5] cars.\n",
        "\n",
        "        Args:\n",
        "            loc_idx (int): the location index. 0 is for A and 1 is for B. All other values are invalid\n",
        "        Returns:\n",
        "            probs (np.ndarray): list of probabilities for all possible combination of s_start and s_end\n",
        "            rewards (np.ndarray): average rewards for all possible s_start\n",
        "        \"\"\"\n",
        "        probs = np.zeros((self.max_cars_start + 1, self.max_cars_end + 1))\n",
        "        rewards = np.zeros(self.max_cars_start + 1)\n",
        "        for start in range(probs.shape[0]):\n",
        "            # TODO Calculate average rewards.\n",
        "            # For all possible s_start, calculate the probability of renting k cars.\n",
        "            # Be sure to consider the case where business is lost (i.e. renting k > s_start cars)\n",
        "            avg_rent = self.rent[loc_idx].mean()\n",
        "            rewards[start] = self.rent_reward * avg_rent\n",
        "\n",
        "            # TODO Calculate probabilities\n",
        "            # Loop over every possible s_end\n",
        "            for end in range(probs.shape[1]):\n",
        "                prob = 0.0\n",
        "                # Since s_start and s_end are specified,\n",
        "                # you must rent a minimum of max(0, start-end)\n",
        "                min_rent = max(0, start - end)\n",
        "\n",
        "                # TODO Loop over all possible rent scenarios and compute probabilities\n",
        "                # Be sure to consider the case where business is lost (i.e. renting k > s_start cars)\n",
        "                for i in range(min_rent, start + 1):\n",
        "                    prob += self.rent[loc_idx].pmf(i)\n",
        "\n",
        "                probs[start, end] = prob\n",
        "\n",
        "        return probs, rewards\n",
        "\n",
        "    def _calculate_cost(self, state: Tuple[int, int], action: int) -> float:\n",
        "        \"\"\"A helper function to compute the cost of moving cars for a given (state, action)\n",
        "\n",
        "        Note that you should compute costs differently if this is the modified problem.\n",
        "\n",
        "        Args:\n",
        "            state (Tuple[int,int]): state\n",
        "            action (int): action\n",
        "        \"\"\"\n",
        "        cost = self.move_cost * action\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def _valid_action(self, state: Tuple[int, int], action: int) -> bool:\n",
        "        \"\"\"Helper function to check if this action is valid for the given state\n",
        "\n",
        "        Args:\n",
        "            state:\n",
        "            action:\n",
        "        \"\"\"\n",
        "        if state[0] < action or state[1] < -(action):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def precompute_transitions(self) -> None:\n",
        "        \"\"\"Function to precompute the transitions and rewards.\n",
        "\n",
        "        This function should have been run at least once before calling expected_return().\n",
        "        You can call this function in __init__() or separately.\n",
        "\n",
        "        \"\"\"\n",
        "        # Calculate open_to_close for each location\n",
        "        day_probs_A, day_rewards_A = self._open_to_close(0)\n",
        "        day_probs_B, day_rewards_B = self._open_to_close(1)\n",
        "\n",
        "        # Perform action first then calculate daytime probabilities\n",
        "        for locA in range(self.max_cars_end + 1):\n",
        "            for locB in range(self.max_cars_end + 1):\n",
        "                for ia, action in enumerate(self.action_space):\n",
        "                    # Check boundary conditions\n",
        "                    if not self._valid_action((locA, locB), action):\n",
        "                        self.t[locA, locB, ia, :, :] = 0\n",
        "                        self.r[locA, locB, ia] = 0\n",
        "                    else:\n",
        "                        # TODO Calculate day rewards from renting\n",
        "                        # Use day_rewards_A and day_rewards_B and _calculate_cost()\n",
        "                        self.r[locA, locB, ia] = day_rewards_A[locA] + day_rewards_B[locB] + \\\n",
        "                                                 self._calculate_cost((locA, locB), action)\n",
        "\n",
        "                        # Loop over all combinations of locA_ and locB_\n",
        "                        for locA_ in range(self.max_cars_end + 1):\n",
        "                            for locB_ in range(self.max_cars_end + 1):\n",
        "\n",
        "                                # TODO Calculate transition probabilities\n",
        "                                # Use the probabilities computed from open_to_close\n",
        "                                self.t[locA, locB, ia, locA_, locB_] = None\n",
        "\n",
        "    def expected_return(\n",
        "        self, V, state: Tuple[int, int], action: Action, gamma: float\n",
        "    ) -> float:\n",
        "        \"\"\"Compute the expected_return for all transitions from the (s,a) pair, i.e. do a 1-step Bellman backup.\n",
        "\n",
        "        Args:\n",
        "            V (np.ndarray): list of state values (length = number of states)\n",
        "            state (Tuple[int, int]): state\n",
        "            action (Action): action\n",
        "            gamma (float): discount factor\n",
        "\n",
        "        Returns:\n",
        "            ret (float): the expected return\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO compute the expected return\n",
        "        ret = None\n",
        "        return ret\n",
        "\n",
        "    def transitions(self, state: Tuple, action: Action) -> np.ndarray:\n",
        "        \"\"\"Get transition probabilities for given (state, action) pair.\n",
        "\n",
        "        Note that this is the 3-argument transition version p(s'|s,a).\n",
        "        This particular environment has stochastic transitions\n",
        "\n",
        "        Args:\n",
        "            state (Tuple): state\n",
        "            action (Action): action\n",
        "\n",
        "        Returns:\n",
        "            probs (np.ndarray): return probabilities for next states. Since transition function is of shape (locA, locB, action, locA', locB'), probs should be of shape (locA', locB')\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        probs = np.zeros((self.max_cars_end + 1, self.max_cars_end + 1))\n",
        "        return probs\n",
        "\n",
        "    def rewards(self, state, action) -> float:\n",
        "        \"\"\"Reward function r(s,a)\n",
        "\n",
        "        Args:\n",
        "            state (Tuple): state\n",
        "            action (Action): action\n",
        "        Returns:\n",
        "            reward: float\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        return action * abs(self.rent_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improve(V, g, pi):\n",
        "    policy_stable = True\n",
        "    for s in g.state_space:\n",
        "        old_action = pi[s]\n",
        "        a = []\n",
        "        for i in range(g.action_space):\n",
        "            a.append(g.expected_return(V, s, i, 0.9))\n",
        "        pi[s] = np.argwhere(a == np.amax(a))\n",
        "\n",
        "        if isinstance(old_action, int):\n",
        "            old_action = np.empty(1)\n",
        "        if old_action.all() != pi[s].all():\n",
        "            policy_stable = False\n",
        "\n",
        "    if policy_stable:\n",
        "        for i in V.T[::-1]:\n",
        "            print(i)\n",
        "\n",
        "        print()\n",
        "        for row in pi.T[::-1]:\n",
        "            col = [a.flatten().tolist() for a in row]\n",
        "            for element in col:\n",
        "                print('[', end='')\n",
        "                for a in element:\n",
        "                    if a == 0:\n",
        "                        print(u\"\\u2190\", end='')  #Left\n",
        "                    if a == 1:\n",
        "                        print(u\"\\u2193\", end='')  #Down\n",
        "                    if a == 2:\n",
        "                        print(u\"\\u2192\", end='')  #Right\n",
        "                    if a == 3:\n",
        "                        print(u\"\\u2191\", end='')  #Up\n",
        "                print('] ', end='')\n",
        "                if len(element) == 1:\n",
        "                    print('   ', end='')\n",
        "                elif len(element) == 2:\n",
        "                    print('  ', end='')\n",
        "            print()\n",
        "        return policy_stable\n",
        "    else:\n",
        "        return policy_stable\n",
        "\n",
        "\n",
        "def policy_eval(V, g, pi):\n",
        "    theta = 0.001\n",
        "    error = 2 * theta\n",
        "\n",
        "    while error > theta:\n",
        "        for s in g.state_space:\n",
        "            v = V[s]\n",
        "            action = pi[s].flatten()[0]\n",
        "            V[s] = 4*g.expected_return(V, s, action, 0.9)\n",
        "\n",
        "\n",
        "            error = abs(v - V[s])\n",
        "\n"
      ],
      "metadata": {
        "id": "mMgsbAFMH3z2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Q5_a():\n",
        "\n",
        "    grid = Gridworld5x5()\n",
        "    value_function = np.zeros(shape=(5, 5))\n",
        "    convergence_threshold = 0.001\n",
        "    error = 2 * convergence_threshold\n",
        "\n",
        "    while error > convergence_threshold:\n",
        "        for state in grid.state_space:\n",
        "            previous_value = value_function[state]\n",
        "\n",
        "            action_returns = 0\n",
        "\n",
        "            # Iterate through possible actions\n",
        "            for action in range(grid.action_space):\n",
        "                action_returns += grid.expected_return(value_function, state, action, 0.9)\n",
        "\n",
        "            # Update the value function\n",
        "            value_function[state] = action_returns\n",
        "            error = abs(previous_value - value_function[state])\n",
        "\n",
        "    # Print the updated value function\n",
        "    print(\"Value function to 1 decimal:-\\n\")\n",
        "    for row in value_function.T[::-1]:\n",
        "        print(row)\n",
        "    print()\n",
        "\n",
        "    # Print the value function with formatting\n",
        "    print(\"Value function:-\\n\")\n",
        "    for row in value_function.T[::-1]:\n",
        "        formatted_row = [f\"{value:.1f}\" for value in row]\n",
        "        print(formatted_row)\n",
        "\n",
        "\n",
        "def Q5_b():\n",
        "\n",
        "    grid = Gridworld5x5()\n",
        "    value_function = np.zeros(shape=(5, 5))\n",
        "    policy = np.empty(shape=(5, 5), dtype=object)\n",
        "    convergence_threshold = 0.001\n",
        "    error = 2 * convergence_threshold\n",
        "\n",
        "    max_iterations = 100\n",
        "\n",
        "    iterations = 0\n",
        "    while iterations < max_iterations:\n",
        "        for state in grid.state_space:\n",
        "            previous_value = value_function[state]\n",
        "\n",
        "            expected_returns = []\n",
        "\n",
        "            for action in range(grid.action_space):\n",
        "                expected_returns.append(4 * grid.expected_return(value_function, state, action, 0.9))\n",
        "\n",
        "            value_function[state] = max(expected_returns)\n",
        "\n",
        "            # Find the best actions\n",
        "            best_actions = np.argwhere(expected_returns == np.amax(expected_returns))\n",
        "            policy[state] = best_actions\n",
        "            error = abs(previous_value - value_function[state])\n",
        "\n",
        "        iterations += 1\n",
        "\n",
        "\n",
        "    for row in value_function.T[::-1]:\n",
        "        print(row)\n",
        "    print()\n",
        "\n",
        "\n",
        "    for row in value_function.T[::-1]:\n",
        "        formatted_row = [f\"{value:.1f}\" for value in row]\n",
        "        print(formatted_row)\n",
        "    print()\n",
        "\n",
        "    # Print the policy with arrows\n",
        "    for row in policy.T[::-1]:\n",
        "        for col in row:\n",
        "            if len(col) == 0:\n",
        "                print('[ ]   ', end='')\n",
        "            else:\n",
        "                print('[', end='')\n",
        "                for action in col:\n",
        "                    if action == 0:\n",
        "                        print(u\"\\u2190\", end='')  # Left\n",
        "                    elif action == 1:\n",
        "                        print(u\"\\u2193\", end='')  # Down\n",
        "                    elif action == 2:\n",
        "                        print(u\"\\u2192\", end='')  # Right\n",
        "                    elif action == 3:\n",
        "                        print(u\"\\u2191\", end='')  # Up\n",
        "                print(']   ', end='')\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "def Q5_c():\n",
        "    grid = Gridworld5x5()\n",
        "    value_function = np.zeros(shape=(5, 5))\n",
        "    policy = np.zeros(shape=(5, 5), dtype=object)\n",
        "    convergence_threshold = 0.1\n",
        "\n",
        "    error = 2 * convergence_threshold\n",
        "\n",
        "    while not policy_improve(value_function, grid, policy):\n",
        "        policy_eval(value_function, grid, policy)\n",
        "\n",
        "\n",
        "# def Q6_a():\n",
        "#     env = JacksCarRental()\n",
        "#     # grid.precompute_transitions()\n",
        "#     threshold = 0.001\n",
        "#     gamma = 0.9\n",
        "#     results_list = run_policy_iteration(env, threshold, gamma)\n",
        "\n",
        "#     # x = grid._open_to_close(1)[0]\n",
        "\n",
        "#     # f = plt.imshow(x)\n",
        "#     # plt.colorbar(f)\n",
        "\n",
        "#     # plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOao08nIH_gv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5_a()\n",
        "# Q5_b()\n",
        "Q5_c()\n",
        "# Q6_a()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GznNby2aIJx6",
        "outputId": "85565197-c60f-457c-b4c6-43527aca7d78"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21.97748529 24.4194281  21.97748529 19.4194281  17.47748529]\n",
            "[19.77973676 21.97748529 19.77973676 17.80176308 16.02158677]\n",
            "[17.80176308 19.77973676 17.80176308 16.02158677 14.4194281 ]\n",
            "[16.02158677 17.80176308 16.02158677 14.4194281  12.97748529]\n",
            "[14.4194281  16.02158677 14.4194281  12.97748529 11.67973676]\n",
            "\n",
            "[→]    [←↓→↑] [←]    [←↓→↑] [←]    \n",
            "[→↑]   [↑]    [←↑]   [←]    [←]    \n",
            "[→↑]   [↑]    [←↑]   [←↑]   [←↑]   \n",
            "[→↑]   [↑]    [←↑]   [←↑]   [←↑]   \n",
            "[→↑]   [↑]    [←↑]   [←↑]   [←↑]   \n"
          ]
        }
      ]
    }
  ]
}